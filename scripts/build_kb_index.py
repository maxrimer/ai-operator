#!/usr/bin/env python3
# build_kb_index.py
"""
1. Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ Ğ²ÑĞµ .docx Ğ¸Ğ· data/kb_docs/
2. Ğ”ĞµĞ»Ğ¸Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸ (Ğ°Ğ±Ğ·Ğ°Ñ†Ñ‹ + ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†) Ñ‡ĞµÑ€ĞµĞ· docx_to_chunks
3. ĞšĞ¸Ğ´Ğ°ĞµÑ‚ Ğ² SentenceTransformer (multilingual-E5-large)
4. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ FAISS-Index + Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ RAG-Ğ¿Ğ¾Ğ¸ÑĞºĞ°
"""

# â”€â”€â”€â”€â”€ std lib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json, re, pathlib, argparse
from typing import List
# â”€â”€â”€â”€â”€ third-party â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import faiss, numpy as np, pandas as pd
from tqdm import tqdm
from docx import Document
from sentence_transformers import SentenceTransformer

# â”€â”€â”€â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOC_DIR   = pathlib.Path("../../../Downloads/AI-ÑÑƒÑ„Ğ»ĞµÑ€ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿/Ğ²ÑĞµ_ĞºĞ±_Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ/")
OUT_DIR   = pathlib.Path("../data/processed")
OUT_META  = pathlib.Path("../data/processed/kb_meta.jsonl")
OUT_INDEX = pathlib.Path("../data/processed/kb_index.index")
MODEL     = "intfloat/multilingual-e5-large"
CHUNK_LEN = 120

# â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOKEN_RE = re.compile(r"\s+")


def clean(txt: str) -> str:
    return TOKEN_RE.sub(" ", txt).strip()


def docx_to_chunks(path: pathlib.Path) -> List[str]:
    """ĞĞ±Ğ·Ğ°Ñ†Ñ‹ + ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, ÑĞºĞ»ĞµĞµĞ½Ğ½Ñ‹Ğµ ' | '."""
    doc = Document(path)
    chunks = []

    # paragraphs
    for p in doc.paragraphs:
        txt = clean(p.text)
        if txt: chunks.append(txt)

    # tables
    for t_idx, tbl in enumerate(doc.tables):
        for r_idx, row in enumerate(tbl.rows):
            cells = [clean(c.text) for c in row.cells if clean(c.text)]
            if cells:
                chunks.append(" | ".join(cells))
    return chunks


def best_device() -> str:
    import torch
    if torch.cuda.is_available(): return "cuda"
    if torch.backends.mps.is_available(): return "mps"
    return "cpu"


# â”€â”€â”€â”€â”€ main build â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build():
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    model = SentenceTransformer(MODEL, device=best_device())

    meta, embeds = [], []
    for doc_path in tqdm(DOC_DIR.glob("*.docx"), desc="â¤µ parsing"):
        for idx, chunk in enumerate(docx_to_chunks(doc_path)):
            words = chunk.split()
            # ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ğ¿Ğ¾ CHUNK_LEN
            for i in range(0, len(words), CHUNK_LEN):
                slice_ = " ".join(words[i:i+CHUNK_LEN])
                if len(slice_.split()) < 20:
                    continue
                emb = model.encode("passage: " + slice_,
                                   normalize_embeddings=True)
                embeds.append(emb)
                meta.append({
                    "doc":    doc_path.stem,
                    "chunk":  slice_,
                    "source": f"{doc_path.name}#{idx}:{i//CHUNK_LEN}"
                })

    vecs = np.asarray(embeds, dtype="float32")
    index = faiss.IndexFlatIP(vecs.shape[1])
    index.add(vecs)

    # â€” save â€”
    faiss.write_index(index, OUT_INDEX)
    with OUT_META.open("w", encoding="utf-8") as f:
        for row in meta:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    print(f"âœ” saved {len(meta)} chunks â†’ {OUT_INDEX}\n"
          f"ğŸ›ˆ Ğ¼ĞµÑ‚Ğ°: {OUT_META}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--rebuild", action="store_true",
                        help="Ğ¿ĞµÑ€ĞµÑĞ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ ĞµÑÑ‚ÑŒ")
    args = parser.parse_args()

    if OUT_INDEX.exists() and OUT_META.exists() and not args.rebuild:
        print("Index already exists. Use --rebuild to overwrite.")
    else:
        build()
